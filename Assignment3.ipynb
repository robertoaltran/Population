{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a1e1cde",
   "metadata": {},
   "source": [
    "## Rent Price Predictor App\n",
    "\n",
    "The real estate market is complex, and often the estimators available on rental sites are not as accurate or accessible. For individuals moving to a new city, especially young people and those looking for room rentals, the task of finding a fair-priced place can be daunting. Recognizing this need, we developed the \"Rent Price Predictor App.\"\n",
    "\n",
    "This digital tool was designed to provide rental price estimates, eliminating the need to rely solely on traditional site estimators. Instead, our app provides a tailor-made solution based on the specific features provided by the user.\n",
    "\n",
    "Created with Streamlit, the \"Rent Price Predictor App\" boasts a user-friendly interface that guides the user through a series of questions about either rooms or houses. Questions range from size and location to available amenities.\n",
    "\n",
    "After collecting the information, the app uses a pre-trained machine learning model to provide a price estimate. We used three renowned machine learning algorithms - Linear Regression, Random Forest, and XGBoost - to ensure accurate and reliable predictions.\n",
    "\n",
    "The code structure is split into main functions:\n",
    "\n",
    "main(): Controls the app interface.\n",
    "room_questions(): Gathers information about rooms.\n",
    "house_questions(): Gathers information about houses.\n",
    "At the end of the process, users receive a price estimate, aiding them in making an informed decision. This is especially valuable for young adults and new city dwellers who might not be familiar with the average prices in the local market.\n",
    "\n",
    "The \"Rent Price Predictor App\" is not just a price prediction tool but a solution designed to make the experience of renting a room or a house more transparent and less intimidating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7acdaf",
   "metadata": {},
   "source": [
    "## data collection\n",
    "how we collect\n",
    "where\n",
    "why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5616973",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe4fc2",
   "metadata": {},
   "source": [
    "## data cleaning and splitting (traing and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26fef0f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/52/xsl0gx4d6rnfwvwr1czp017w0000gn/T/ipykernel_80916/3142729961.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X = df.drop('target_column', axis=1)\n",
    "y = df['target_column']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d2c7c0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35dbeb3d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/52/xsl0gx4d6rnfwvwr1czp017w0000gn/T/ipykernel_80916/2351163572.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target_column'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_column'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_dataset.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "results = {} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c7e58",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Linear regression is a simple yet powerful statistical technique used to model and analyze the relationships between two variables. With roots tracing back to the work of the English polymath Sir Francis Galton, linear regression has been a foundational tool in statistics and machine learning.\n",
    "\n",
    "At its core, linear regression attempts to find the best straight line that describes the relationship between an independent (or predictor) variable and a dependent variable. This \"best straight line\" is determined using the least squares method, which tries to minimize the sum of the squares of the errors between the observed data points and the points predicted by the regression line.\n",
    "\n",
    "The equation for this straight line is usually expressed as y=mx+b, where:\n",
    "\n",
    "y is the dependent variable or what we are trying to predict.\n",
    "\n",
    "x is the independent variable or the predictor.\n",
    "\n",
    "m is the slope of the line.\n",
    "\n",
    "b is the y-axis intercept.\n",
    "\n",
    "Implementing the Linear Regression Function:\n",
    "\n",
    "In the context of the provided code, the function linear_regression_tuning() carries out the following steps:\n",
    "\n",
    "Preparation: Initially, the function sets up a linear regression model.\n",
    "Feature Selection: It uses the \"step-forward\" method, a sequential selection technique where we start with no features and keep adding those that provide the best performance boost for the model.\n",
    "Training: With the best features selected, the model is trained using the training data.\n",
    "Prediction and Evaluation: The trained model is used to make predictions on the test data. The model's performance is then assessed by computing the Mean Squared Error (MSE) between the predictions and the true values.\n",
    "Return: The function returns the trained model, the MSE, and the best-selected features.\n",
    "This approach provides a robust way to select the most relevant features for linear regression and train a model capable of making accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_forward_regression(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Seleção step-forward\n",
    "    sfs = SFS(LinearRegression(),\n",
    "              k_features='best',\n",
    "              forward=True,\n",
    "              scoring='neg_mean_squared_error',\n",
    "              cv=5)\n",
    "\n",
    "    sfs = sfs.fit(X_train, y_train)\n",
    "    \n",
    "    selected_features = sfs.k_feature_names_\n",
    "\n",
    "    # Treinar modelo com as características selecionadas\n",
    "    model = LinearRegression().fit(X_train[list(selected_features)], y_train)\n",
    "    \n",
    "    # Avaliação do modelo\n",
    "    y_pred = model.predict(X_test[list(selected_features)])\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Selected Features: {selected_features}\")\n",
    "    print(f\"MSE: {mse}\")\n",
    "    \n",
    "    return {\"model\": model, \"mse\": mse, \"features\": selected_features}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b1b6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results[\"Step_Forward_Regression\"] = step_forward_regression(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9a2fe",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Random Forests are a machine learning method utilized for tasks such as classification and regression. They function by constructing a \"forest\" of decision trees during training and generate predictions based on the mode (for classification) or mean (for regression) of individual trees' outputs.\n",
    "\n",
    "A few key characteristics and workings of Random Forest include:\n",
    "\n",
    "Random Splitting Method: Random forests, unlike standard decision trees, utilize a random subset of features for each split. This method introduces diversity, making the model more robust and less prone to overfitting.\n",
    "\n",
    "Combining Trees: Multiple trees are built, and the final prediction is an aggregation, either by majority voting (classification) or averaging (regression).\n",
    "\n",
    "Out-of-Bag (OOB) Estimation: Trees are trained on bootstrapped samples, and the unselected samples, called OOB samples, gauge the model's generalization error.\n",
    "\n",
    "Feature Importance: Random forests can indicate the significance of variables by assessing the drop in accuracy due to permutations of each variable's values.\n",
    "\n",
    "Key hyperparameters in Random Forest include:\n",
    "\n",
    "n_estimators: Number of trees to be constructed.\n",
    "\n",
    "max_features: Maximum number of features considered during a split.\n",
    "\n",
    "max_depth: Maximum tree depth.\n",
    "\n",
    "min_samples_split and min_samples_leaf: Determine node splitting and leaf node creation.\n",
    "\n",
    "bootstrap: Whether to use bootstrap sampling.\n",
    "\n",
    "The inception of random forests traces back to Leo Breiman, combining methods similar to CART with random node optimization and bagging.\n",
    "\n",
    "For hyperparameter tuning, a function like random_forest_tuning() can utilize techniques like RandomizedSearchCV to search for optimal hyperparameters. The function would assess hyperparameters such as n_estimators, max_features, and max_depth among others to optimize the Random Forest model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e787d90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def random_forest_tuning(X_train, y_train, X_test, y_test):\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    \n",
    "    param_dist = {\n",
    "        'n_estimators': randint(50, 200),\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth': randint(1, 40)\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=100, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error', random_state=42)\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    best_random_params = random_search.best_params_\n",
    "\n",
    "    best_random = random_search.best_estimator_\n",
    "    random_pred = best_random.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, random_pred)\n",
    "\n",
    "    print(f'Best Parameters: {best_random_params}')\n",
    "    print(f'MSE of Best Model: {mse}')\n",
    "    \n",
    "    return {\"model\": best_random, \"mse\": mse, \"best_params\": best_random_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235fa98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"Random_Forest\"] = random_forest_tuning(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e80c83a",
   "metadata": {},
   "source": [
    "## XGBoost Algorithm\n",
    "\n",
    "XGBoost is a decision-tree based machine learning algorithm that leverages the Gradient boosting framework. While artificial neural networks excel in problems involving unstructured data like images and texts, decision-tree based algorithms are found to be highly efficient for structured data scenarios.\n",
    "\n",
    "Key points about XGBoost:\n",
    "\n",
    "Origin: The algorithm was developed at the University of Washington by Tianqi Chen and Carlos Guestrin and gained prominence on platforms like Kaggle.\n",
    "Unique Features:\n",
    "Versatile across various applications, useful for regression, classification, and more.\n",
    "Portable across various operating systems.\n",
    "Supports multiple programming languages.\n",
    "Seamlessly integrates with cloud platforms and other ecosystems.\n",
    "Functionality: Decision trees are intuitive by design, but XGBoost enhances this concept. It's akin to an optimized interview process where various criteria are tweaked and merged to yield the best outcome. Specifically, XGBoost is like supercharged gradient boosting, combining optimization techniques to achieve better results with fewer resources.\n",
    "Effectiveness: XGBoost and Gradient Boosting Machines (GBMs) are tree-based methods that apply the boosting principle. XGBoost, however, optimizes GBM, enhancing both system efficiency and algorithmic accuracy.\n",
    "In summary, XGBoost is a sophisticated, adaptable algorithm that stands out in a variety of machine learning scenarios, especially with structured data.\n",
    "\n",
    "For hyperparameter tuning in XGBoost, functions like xgboost_tuning() can be applyed. This functions typically assess and tune parameters like learning rate, max depth of trees, and regularization terms, among others. These adjustments ensure optimal model performance while leveraging the unique capabilities of the XGBoost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd7cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def xgboost_tuning(X_train, y_train, X_test, y_test):\n",
    "    xg = xgb.XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(50, 200),\n",
    "        'learning_rate': uniform(0.01, 0.6),\n",
    "        'max_depth': randint(1, 40),\n",
    "        'gamma': uniform(0, 0.5),\n",
    "        'colsample_bytree': uniform(0.5, 0.9)\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(estimator=xg, param_distributions=param_dist, n_iter=100, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error', random_state=42)\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    best_random_params = random_search.best_params_\n",
    "\n",
    "    best_random = random_search.best_estimator_\n",
    "    random_pred = best_random.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, random_pred)\n",
    "\n",
    "    print(f'Best Parameters: {best_random_params}')\n",
    "    print(f'MSE of Best Model: {mse}')\n",
    "\n",
    "    return {\"model\": best_random, \"mse\": mse, \"best_params\": best_random_params}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"XGBoost\"] = xgboost_tuning(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5925114d",
   "metadata": {},
   "source": [
    "## Using Mean Squared Error (MSE) for Model Selection\n",
    "\n",
    "When developing machine learning models, it is crucial to use evaluation metrics to determine a model's performance. For regression models, the Mean Squared Error (MSE) is a widely recognized and utilized metric.\n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "MSE is used to measure the average of the squares of the errors between predictions and actual observations. Mathematically, it's expressed by the formula:\n",
    "\n",
    "\\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{actual},i} - y_{\\text{predicted},i})^2 \\]\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( n \\) is the total number of observations.\n",
    "- \\( y_{\\text{actual},i} \\) is the actual value of the i-th observation.\n",
    "- \\( y_{\\text{predicted},i} \\) is the value predicted by the model for the i-th observation.\n",
    "\n",
    "**Evaluating the Model Based on MSE**\n",
    "\n",
    "To choose the best model among various candidates, it's common to select the one with the lowest MSE. In the project context, after applying step-by-step regression, Random Forest, and XGBoost, the model with the lowest MSE was selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be33fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
